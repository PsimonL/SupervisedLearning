{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        CRIM   ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n496  0.28960  0.0   9.69     0  0.585  5.390  72.9  2.7986    6  391.0   \n497  0.26838  0.0   9.69     0  0.585  5.794  70.6  2.8927    6  391.0   \n498  0.23912  0.0   9.69     0  0.585  6.019  65.3  2.4091    6  391.0   \n499  0.17783  0.0   9.69     0  0.585  5.569  73.5  2.3999    6  391.0   \n500  0.22438  0.0   9.69     0  0.585  6.027  79.7  2.4982    6  391.0   \n501  0.06263  0.0  11.93     0  0.573  6.593  69.1  2.4786    1  273.0   \n502  0.04527  0.0  11.93     0  0.573  6.120  76.7  2.2875    1  273.0   \n503  0.06076  0.0  11.93     0  0.573  6.976  91.0  2.1675    1  273.0   \n504  0.10959  0.0  11.93     0  0.573  6.794  89.3  2.3889    1  273.0   \n505  0.04741  0.0  11.93     0  0.573  6.030  80.8  2.5050    1  273.0   \n\n     PTRATIO       B  LSTAT  MEDV  \n496     19.2  396.90  21.14  19.7  \n497     19.2  396.90  14.10  18.3  \n498     19.2  396.90  12.92  21.2  \n499     19.2  395.77  15.10  17.5  \n500     19.2  396.90  14.33  16.8  \n501     21.0  391.99   9.67  22.4  \n502     21.0  396.90   9.08  20.6  \n503     21.0  396.90   5.64  23.9  \n504     21.0  393.45   6.48  22.0  \n505     21.0  396.90   7.88  11.9  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>CRIM</th>\n      <th>ZN</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>NOX</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>RAD</th>\n      <th>TAX</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>MEDV</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>496</th>\n      <td>0.28960</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.390</td>\n      <td>72.9</td>\n      <td>2.7986</td>\n      <td>6</td>\n      <td>391.0</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>21.14</td>\n      <td>19.7</td>\n    </tr>\n    <tr>\n      <th>497</th>\n      <td>0.26838</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.794</td>\n      <td>70.6</td>\n      <td>2.8927</td>\n      <td>6</td>\n      <td>391.0</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>14.10</td>\n      <td>18.3</td>\n    </tr>\n    <tr>\n      <th>498</th>\n      <td>0.23912</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>6.019</td>\n      <td>65.3</td>\n      <td>2.4091</td>\n      <td>6</td>\n      <td>391.0</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>12.92</td>\n      <td>21.2</td>\n    </tr>\n    <tr>\n      <th>499</th>\n      <td>0.17783</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>5.569</td>\n      <td>73.5</td>\n      <td>2.3999</td>\n      <td>6</td>\n      <td>391.0</td>\n      <td>19.2</td>\n      <td>395.77</td>\n      <td>15.10</td>\n      <td>17.5</td>\n    </tr>\n    <tr>\n      <th>500</th>\n      <td>0.22438</td>\n      <td>0.0</td>\n      <td>9.69</td>\n      <td>0</td>\n      <td>0.585</td>\n      <td>6.027</td>\n      <td>79.7</td>\n      <td>2.4982</td>\n      <td>6</td>\n      <td>391.0</td>\n      <td>19.2</td>\n      <td>396.90</td>\n      <td>14.33</td>\n      <td>16.8</td>\n    </tr>\n    <tr>\n      <th>501</th>\n      <td>0.06263</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.593</td>\n      <td>69.1</td>\n      <td>2.4786</td>\n      <td>1</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>391.99</td>\n      <td>9.67</td>\n      <td>22.4</td>\n    </tr>\n    <tr>\n      <th>502</th>\n      <td>0.04527</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.120</td>\n      <td>76.7</td>\n      <td>2.2875</td>\n      <td>1</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>9.08</td>\n      <td>20.6</td>\n    </tr>\n    <tr>\n      <th>503</th>\n      <td>0.06076</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.976</td>\n      <td>91.0</td>\n      <td>2.1675</td>\n      <td>1</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>5.64</td>\n      <td>23.9</td>\n    </tr>\n    <tr>\n      <th>504</th>\n      <td>0.10959</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.794</td>\n      <td>89.3</td>\n      <td>2.3889</td>\n      <td>1</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>393.45</td>\n      <td>6.48</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>505</th>\n      <td>0.04741</td>\n      <td>0.0</td>\n      <td>11.93</td>\n      <td>0</td>\n      <td>0.573</td>\n      <td>6.030</td>\n      <td>80.8</td>\n      <td>2.5050</td>\n      <td>1</td>\n      <td>273.0</td>\n      <td>21.0</td>\n      <td>396.90</td>\n      <td>7.88</td>\n      <td>11.9</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df = pd.read_csv(r\"housing.data\", sep=\" +\", engine=\"python\", header=None, names=[\"CRIM\",\"ZN\",\"INDUS\",\"CHAS\",\"NOX\",\"RM\",\"AGE\",\"DIS\",\"RAD\",\"TAX\",\"PTRATIO\",\"B\",\"LSTAT\",\"MEDV\"])\n",
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 4.98],\n       [ 9.14],\n       [ 4.03],\n       [ 2.94],\n       [ 5.33],\n       [ 5.21],\n       [12.43],\n       [19.15],\n       [29.93],\n       [17.1 ],\n       [20.45],\n       [13.27],\n       [15.71],\n       [ 8.26],\n       [10.26],\n       [ 8.47],\n       [ 6.58],\n       [14.67],\n       [11.69],\n       [11.28],\n       [21.02],\n       [13.83],\n       [18.72],\n       [19.88],\n       [16.3 ],\n       [16.51],\n       [14.81],\n       [17.28],\n       [12.8 ],\n       [11.98],\n       [22.6 ],\n       [13.04],\n       [27.71],\n       [18.35],\n       [20.34],\n       [ 9.68],\n       [11.41],\n       [ 8.77],\n       [10.13],\n       [ 4.32],\n       [ 1.98],\n       [ 4.84],\n       [ 5.81],\n       [ 7.44],\n       [ 9.55],\n       [10.21],\n       [14.15],\n       [18.8 ],\n       [30.81],\n       [16.2 ],\n       [13.45],\n       [ 9.43],\n       [ 5.28],\n       [ 8.43],\n       [14.8 ],\n       [ 4.81],\n       [ 5.77],\n       [ 3.95],\n       [ 6.86],\n       [ 9.22],\n       [13.15],\n       [14.44],\n       [ 6.73],\n       [ 9.5 ],\n       [ 8.05],\n       [ 4.67],\n       [10.24],\n       [ 8.1 ],\n       [13.09],\n       [ 8.79],\n       [ 6.72],\n       [ 9.88],\n       [ 5.52],\n       [ 7.54],\n       [ 6.78],\n       [ 8.94],\n       [11.97],\n       [10.27],\n       [12.34],\n       [ 9.1 ],\n       [ 5.29],\n       [ 7.22],\n       [ 6.72],\n       [ 7.51],\n       [ 9.62],\n       [ 6.53],\n       [12.86],\n       [ 8.44],\n       [ 5.5 ],\n       [ 5.7 ],\n       [ 8.81],\n       [ 8.2 ],\n       [ 8.16],\n       [ 6.21],\n       [10.59],\n       [ 6.65],\n       [11.34],\n       [ 4.21],\n       [ 3.57],\n       [ 6.19],\n       [ 9.42],\n       [ 7.67],\n       [10.63],\n       [13.44],\n       [12.33],\n       [16.47],\n       [18.66],\n       [14.09],\n       [12.27],\n       [15.55],\n       [13.  ],\n       [10.16],\n       [16.21],\n       [17.09],\n       [10.45],\n       [15.76],\n       [12.04],\n       [10.3 ],\n       [15.37],\n       [13.61],\n       [14.37],\n       [14.27],\n       [17.93],\n       [25.41],\n       [17.58],\n       [14.81],\n       [27.26],\n       [17.19],\n       [15.39],\n       [18.34],\n       [12.6 ],\n       [12.26],\n       [11.12],\n       [15.03],\n       [17.31],\n       [16.96],\n       [16.9 ],\n       [14.59],\n       [21.32],\n       [18.46],\n       [24.16],\n       [34.41],\n       [26.82],\n       [26.42],\n       [29.29],\n       [27.8 ],\n       [16.65],\n       [29.53],\n       [28.32],\n       [21.45],\n       [14.1 ],\n       [13.28],\n       [12.12],\n       [15.79],\n       [15.12],\n       [15.02],\n       [16.14],\n       [ 4.59],\n       [ 6.43],\n       [ 7.39],\n       [ 5.5 ],\n       [ 1.73],\n       [ 1.92],\n       [ 3.32],\n       [11.64],\n       [ 9.81],\n       [ 3.7 ],\n       [12.14],\n       [11.1 ],\n       [11.32],\n       [14.43],\n       [12.03],\n       [14.69],\n       [ 9.04],\n       [ 9.64],\n       [ 5.33],\n       [10.11],\n       [ 6.29],\n       [ 6.92],\n       [ 5.04],\n       [ 7.56],\n       [ 9.45],\n       [ 4.82],\n       [ 5.68],\n       [13.98],\n       [13.15],\n       [ 4.45],\n       [ 6.68],\n       [ 4.56],\n       [ 5.39],\n       [ 5.1 ],\n       [ 4.69],\n       [ 2.87],\n       [ 5.03],\n       [ 4.38],\n       [ 2.97],\n       [ 4.08],\n       [ 8.61],\n       [ 6.62],\n       [ 4.56],\n       [ 4.45],\n       [ 7.43],\n       [ 3.11],\n       [ 3.81],\n       [ 2.88],\n       [10.87],\n       [10.97],\n       [18.06],\n       [14.66],\n       [23.09],\n       [17.27],\n       [23.98],\n       [16.03],\n       [ 9.38],\n       [29.55],\n       [ 9.47],\n       [13.51],\n       [ 9.69],\n       [17.92],\n       [10.5 ],\n       [ 9.71],\n       [21.46],\n       [ 9.93],\n       [ 7.6 ],\n       [ 4.14],\n       [ 4.63],\n       [ 3.13],\n       [ 6.36],\n       [ 3.92],\n       [ 3.76],\n       [11.65],\n       [ 5.25],\n       [ 2.47],\n       [ 3.95],\n       [ 8.05],\n       [10.88],\n       [ 9.54],\n       [ 4.73],\n       [ 6.36],\n       [ 7.37],\n       [11.38],\n       [12.4 ],\n       [11.22],\n       [ 5.19],\n       [12.5 ],\n       [18.46],\n       [ 9.16],\n       [10.15],\n       [ 9.52],\n       [ 6.56],\n       [ 5.9 ],\n       [ 3.59],\n       [ 3.53],\n       [ 3.54],\n       [ 6.57],\n       [ 9.25],\n       [ 3.11],\n       [ 5.12],\n       [ 7.79],\n       [ 6.9 ],\n       [ 9.59],\n       [ 7.26],\n       [ 5.91],\n       [11.25],\n       [ 8.1 ],\n       [10.45],\n       [14.79],\n       [ 7.44],\n       [ 3.16],\n       [13.65],\n       [13.  ],\n       [ 6.59],\n       [ 7.73],\n       [ 6.58],\n       [ 3.53],\n       [ 2.98],\n       [ 6.05],\n       [ 4.16],\n       [ 7.19],\n       [ 4.85],\n       [ 3.76],\n       [ 4.59],\n       [ 3.01],\n       [ 3.16],\n       [ 7.85],\n       [ 8.23],\n       [12.93],\n       [ 7.14],\n       [ 7.6 ],\n       [ 9.51],\n       [ 3.33],\n       [ 3.56],\n       [ 4.7 ],\n       [ 8.58],\n       [10.4 ],\n       [ 6.27],\n       [ 7.39],\n       [15.84],\n       [ 4.97],\n       [ 4.74],\n       [ 6.07],\n       [ 9.5 ],\n       [ 8.67],\n       [ 4.86],\n       [ 6.93],\n       [ 8.93],\n       [ 6.47],\n       [ 7.53],\n       [ 4.54],\n       [ 9.97],\n       [12.64],\n       [ 5.98],\n       [11.72],\n       [ 7.9 ],\n       [ 9.28],\n       [11.5 ],\n       [18.33],\n       [15.94],\n       [10.36],\n       [12.73],\n       [ 7.2 ],\n       [ 6.87],\n       [ 7.7 ],\n       [11.74],\n       [ 6.12],\n       [ 5.08],\n       [ 6.15],\n       [12.79],\n       [ 9.97],\n       [ 7.34],\n       [ 9.09],\n       [12.43],\n       [ 7.83],\n       [ 5.68],\n       [ 6.75],\n       [ 8.01],\n       [ 9.8 ],\n       [10.56],\n       [ 8.51],\n       [ 9.74],\n       [ 9.29],\n       [ 5.49],\n       [ 8.65],\n       [ 7.18],\n       [ 4.61],\n       [10.53],\n       [12.67],\n       [ 6.36],\n       [ 5.99],\n       [ 5.89],\n       [ 5.98],\n       [ 5.49],\n       [ 7.79],\n       [ 4.5 ],\n       [ 8.05],\n       [ 5.57],\n       [17.6 ],\n       [13.27],\n       [11.48],\n       [12.67],\n       [ 7.79],\n       [14.19],\n       [10.19],\n       [14.64],\n       [ 5.29],\n       [ 7.12],\n       [14.  ],\n       [13.33],\n       [ 3.26],\n       [ 3.73],\n       [ 2.96],\n       [ 9.53],\n       [ 8.88],\n       [34.77],\n       [37.97],\n       [13.44],\n       [23.24],\n       [21.24],\n       [23.69],\n       [21.78],\n       [17.21],\n       [21.08],\n       [23.6 ],\n       [24.56],\n       [30.63],\n       [30.81],\n       [28.28],\n       [31.99],\n       [30.62],\n       [20.85],\n       [17.11],\n       [18.76],\n       [25.68],\n       [15.17],\n       [16.35],\n       [17.12],\n       [19.37],\n       [19.92],\n       [30.59],\n       [29.97],\n       [26.77],\n       [20.32],\n       [20.31],\n       [19.77],\n       [27.38],\n       [22.98],\n       [23.34],\n       [12.13],\n       [26.4 ],\n       [19.78],\n       [10.11],\n       [21.22],\n       [34.37],\n       [20.08],\n       [36.98],\n       [29.05],\n       [25.79],\n       [26.64],\n       [20.62],\n       [22.74],\n       [15.02],\n       [15.7 ],\n       [14.1 ],\n       [23.29],\n       [17.16],\n       [24.39],\n       [15.69],\n       [14.52],\n       [21.52],\n       [24.08],\n       [17.64],\n       [19.69],\n       [12.03],\n       [16.22],\n       [15.17],\n       [23.27],\n       [18.05],\n       [26.45],\n       [34.02],\n       [22.88],\n       [22.11],\n       [19.52],\n       [16.59],\n       [18.85],\n       [23.79],\n       [23.98],\n       [17.79],\n       [16.44],\n       [18.13],\n       [19.31],\n       [17.44],\n       [17.73],\n       [17.27],\n       [16.74],\n       [18.71],\n       [18.13],\n       [19.01],\n       [16.94],\n       [16.23],\n       [14.7 ],\n       [16.42],\n       [14.65],\n       [13.99],\n       [10.29],\n       [13.22],\n       [14.13],\n       [17.15],\n       [21.32],\n       [18.13],\n       [14.76],\n       [16.29],\n       [12.87],\n       [14.36],\n       [11.66],\n       [18.14],\n       [24.1 ],\n       [18.68],\n       [24.91],\n       [18.03],\n       [13.11],\n       [10.74],\n       [ 7.74],\n       [ 7.01],\n       [10.42],\n       [13.34],\n       [10.58],\n       [14.98],\n       [11.45],\n       [18.06],\n       [23.97],\n       [29.68],\n       [18.07],\n       [13.35],\n       [12.01],\n       [13.59],\n       [17.6 ],\n       [21.14],\n       [14.1 ],\n       [12.92],\n       [15.1 ],\n       [14.33],\n       [ 9.67],\n       [ 9.08],\n       [ 5.64],\n       [ 6.48],\n       [ 7.88]])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.loc[:, \"LSTAT\"].values.reshape(-1,1)\n",
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9, 15. ,\n       18.9, 21.7, 20.4, 18.2, 19.9, 23.1, 17.5, 20.2, 18.2, 13.6, 19.6,\n       15.2, 14.5, 15.6, 13.9, 16.6, 14.8, 18.4, 21. , 12.7, 14.5, 13.2,\n       13.1, 13.5, 18.9, 20. , 21. , 24.7, 30.8, 34.9, 26.6, 25.3, 24.7,\n       21.2, 19.3, 20. , 16.6, 14.4, 19.4, 19.7, 20.5, 25. , 23.4, 18.9,\n       35.4, 24.7, 31.6, 23.3, 19.6, 18.7, 16. , 22.2, 25. , 33. , 23.5,\n       19.4, 22. , 17.4, 20.9, 24.2, 21.7, 22.8, 23.4, 24.1, 21.4, 20. ,\n       20.8, 21.2, 20.3, 28. , 23.9, 24.8, 22.9, 23.9, 26.6, 22.5, 22.2,\n       23.6, 28.7, 22.6, 22. , 22.9, 25. , 20.6, 28.4, 21.4, 38.7, 43.8,\n       33.2, 27.5, 26.5, 18.6, 19.3, 20.1, 19.5, 19.5, 20.4, 19.8, 19.4,\n       21.7, 22.8, 18.8, 18.7, 18.5, 18.3, 21.2, 19.2, 20.4, 19.3, 22. ,\n       20.3, 20.5, 17.3, 18.8, 21.4, 15.7, 16.2, 18. , 14.3, 19.2, 19.6,\n       23. , 18.4, 15.6, 18.1, 17.4, 17.1, 13.3, 17.8, 14. , 14.4, 13.4,\n       15.6, 11.8, 13.8, 15.6, 14.6, 17.8, 15.4, 21.5, 19.6, 15.3, 19.4,\n       17. , 15.6, 13.1, 41.3, 24.3, 23.3, 27. , 50. , 50. , 50. , 22.7,\n       25. , 50. , 23.8, 23.8, 22.3, 17.4, 19.1, 23.1, 23.6, 22.6, 29.4,\n       23.2, 24.6, 29.9, 37.2, 39.8, 36.2, 37.9, 32.5, 26.4, 29.6, 50. ,\n       32. , 29.8, 34.9, 37. , 30.5, 36.4, 31.1, 29.1, 50. , 33.3, 30.3,\n       34.6, 34.9, 32.9, 24.1, 42.3, 48.5, 50. , 22.6, 24.4, 22.5, 24.4,\n       20. , 21.7, 19.3, 22.4, 28.1, 23.7, 25. , 23.3, 28.7, 21.5, 23. ,\n       26.7, 21.7, 27.5, 30.1, 44.8, 50. , 37.6, 31.6, 46.7, 31.5, 24.3,\n       31.7, 41.7, 48.3, 29. , 24. , 25.1, 31.5, 23.7, 23.3, 22. , 20.1,\n       22.2, 23.7, 17.6, 18.5, 24.3, 20.5, 24.5, 26.2, 24.4, 24.8, 29.6,\n       42.8, 21.9, 20.9, 44. , 50. , 36. , 30.1, 33.8, 43.1, 48.8, 31. ,\n       36.5, 22.8, 30.7, 50. , 43.5, 20.7, 21.1, 25.2, 24.4, 35.2, 32.4,\n       32. , 33.2, 33.1, 29.1, 35.1, 45.4, 35.4, 46. , 50. , 32.2, 22. ,\n       20.1, 23.2, 22.3, 24.8, 28.5, 37.3, 27.9, 23.9, 21.7, 28.6, 27.1,\n       20.3, 22.5, 29. , 24.8, 22. , 26.4, 33.1, 36.1, 28.4, 33.4, 28.2,\n       22.8, 20.3, 16.1, 22.1, 19.4, 21.6, 23.8, 16.2, 17.8, 19.8, 23.1,\n       21. , 23.8, 23.1, 20.4, 18.5, 25. , 24.6, 23. , 22.2, 19.3, 22.6,\n       19.8, 17.1, 19.4, 22.2, 20.7, 21.1, 19.5, 18.5, 20.6, 19. , 18.7,\n       32.7, 16.5, 23.9, 31.2, 17.5, 17.2, 23.1, 24.5, 26.6, 22.9, 24.1,\n       18.6, 30.1, 18.2, 20.6, 17.8, 21.7, 22.7, 22.6, 25. , 19.9, 20.8,\n       16.8, 21.9, 27.5, 21.9, 23.1, 50. , 50. , 50. , 50. , 50. , 13.8,\n       13.8, 15. , 13.9, 13.3, 13.1, 10.2, 10.4, 10.9, 11.3, 12.3,  8.8,\n        7.2, 10.5,  7.4, 10.2, 11.5, 15.1, 23.2,  9.7, 13.8, 12.7, 13.1,\n       12.5,  8.5,  5. ,  6.3,  5.6,  7.2, 12.1,  8.3,  8.5,  5. , 11.9,\n       27.9, 17.2, 27.5, 15. , 17.2, 17.9, 16.3,  7. ,  7.2,  7.5, 10.4,\n        8.8,  8.4, 16.7, 14.2, 20.8, 13.4, 11.7,  8.3, 10.2, 10.9, 11. ,\n        9.5, 14.5, 14.1, 16.1, 14.3, 11.7, 13.4,  9.6,  8.7,  8.4, 12.8,\n       10.5, 17.1, 18.4, 15.4, 10.8, 11.8, 14.9, 12.6, 14.1, 13. , 13.4,\n       15.2, 16.1, 17.8, 14.9, 14.1, 12.7, 13.5, 14.9, 20. , 16.4, 17.7,\n       19.5, 20.2, 21.4, 19.9, 19. , 19.1, 19.1, 20.1, 19.9, 19.6, 23.2,\n       29.8, 13.8, 13.3, 16.7, 12. , 14.6, 21.4, 23. , 23.7, 25. , 21.8,\n       20.6, 21.2, 19.1, 20.6, 15.2,  7. ,  8.1, 13.6, 20.1, 21.8, 24.5,\n       23.1, 19.7, 18.3, 21.2, 17.5, 16.8, 22.4, 20.6, 23.9, 22. , 11.9])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df[\"MEDV\"].values\n",
    "y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "scaler = StandardScaler().fit(X)\n",
    "X_scaled = scaler.transform(X)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ransac lin - reg model R2 value = 0.45995143677443395\n"
     ]
    }
   ],
   "source": [
    "ransac = RANSACRegressor(residual_threshold=20).fit(X_train, y_train)\n",
    "r2_ransac = r2_score(y_test, ransac.predict(X_test))\n",
    "print(f\"RANSAC lin - reg model R2 value = {r2_ransac}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression result: 0.46486358575168896\n"
     ]
    }
   ],
   "source": [
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "r2_lin_reg = r2_score(y_test, lin_reg.predict(X_test))\n",
    "print(f\"Lin - Reg model R2 value = {r2_lin_reg}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Classic Lin Reg model got better result. Aiming for threshold that will outmatch RANSAC as better one."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}